{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to: Find and Access EMIT Data\n",
    "\n",
    "**Summary**  \n",
    "\n",
    "This notebook will explain how to access Earth Surface Minteral Dust Source Investigation (EMIT) data programmaticly using NASA's CMR API. The Common Metadata Repository (CMR) is a metadata system that catalogs Earth Science data and associated metadata records. The CMR Application Programming Interface (API) provides programatic search capabilities through CMR's vast metadata holdings using various parameters and keywords. When querying NASA's CMR, there is a limit of 1 million granules matched and only 2000 granules returned per page. \n",
    "\n",
    "**Requirements:**\n",
    "+ A NASA [Earthdata Login](https://urs.earthdata.nasa.gov/) account is required to download EMIT data   \n",
    "+ Selected the `emit_tutorials` environment as the kernel for this notebook.\n",
    "  + For instructions on setting up the environment, follow the the `setup_instructions.md` included in the `/setup/` folder of the repository.  \n",
    "\n",
    "**Learning Objectives**  \n",
    "- How to find EMIT data using NASA's CMR API\n",
    "- How to download programmatically "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import earthaccess\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import geopandas\n",
    "from shapely.geometry import MultiPolygon, Polygon, box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.14.4)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: numpy>=1.22 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopandas) (1.26.3)\n",
      "Requirement already satisfied: packaging in /Users/arpitasen/Library/Python/3.12/lib/python/site-packages (from geopandas) (23.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopandas) (3.6.1)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopandas) (2.0.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fiona>=1.8.21->geopandas) (23.2.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fiona>=1.8.21->geopandas) (2024.2.2)\n",
      "Requirement already satisfied: click~=8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fiona>=1.8.21->geopandas) (8.1.7)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fiona>=1.8.21->geopandas) (1.1.1)\n",
      "Requirement already satisfied: cligj>=0.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fiona>=1.8.21->geopandas) (0.7.2)\n",
      "Requirement already satisfied: six in /Users/arpitasen/Library/Python/3.12/lib/python/site-packages (from fiona>=1.8.21->geopandas) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/arpitasen/Library/Python/3.12/lib/python/site-packages (from pandas>=1.4.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=1.4.0->geopandas) (2024.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Concept ID\n",
    "\n",
    "NASA EarthData's unique ID for this dataset (called Concept ID) is needed for searching the dataset. The dataset Digital Object Identifier or DOI can be used to obtain the Concept ID. DOIs can be found by clicking the `Citation` link on the LP DAAC's [EMIT Product Pages](https://lpdaac.usgs.gov/product_search/?query=emit&view=cards&sort=title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2408750690-LPCLOUD\n"
     ]
    }
   ],
   "source": [
    "# 10.5067/ASTER/AST_08.003\n",
    "doi = '10.5067/EMIT/EMITL2ARFL.001'# EMIT L2A Reflectance\n",
    "\n",
    "# CMR API base url\n",
    "cmrurl='https://cmr.earthdata.nasa.gov/search/' \n",
    "\n",
    "doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "concept_id = requests.get(doisearch).json()['feed']['entry'][0]['id']\n",
    "print(concept_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the unique NASA-given concept ID for the EMIT L2A Reflectance dataset, which can be used to retrieve relevant files (or granules)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching using CMR API\n",
    "\n",
    "When searching the CMR API, users can provide spatial bounds and date-time ranges to narrow their search. These spatial bounds can be either, points, a bounding box, or a polygon. \n",
    "\n",
    "Specify start time and dates and reformat them to the structure necessary for searching CMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-03T00:00:00Z,2022-09-03T23:23:59Z\n"
     ]
    }
   ],
   "source": [
    "# Temporal Bound - Year, month, day. Hour, minutes, and seconds (ZULU) can also be included \n",
    "start_date = dt.datetime(2022, 9, 3)\n",
    "end_date = dt.datetime(2024, 3, 3, 23, 23, 59)  \n",
    "\n",
    "# CMR formatted start and end times\n",
    "dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "temporal_str = start_date.strftime(dt_format) + ',' + end_date.strftime(dt_format)\n",
    "print(temporal_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMR API only allows 2000 results to be shown at a time. Using `page_num` allows a user to loop through the search result pages. The sections below walk through using Points, Bounding Boxes, and Polygons to spatially constrain a search made using the CMR API. \n",
    "\n",
    "### Search using Points\n",
    "\n",
    "To search using a point we specify a latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "[['-39.242054 -62.0887375 -39.9433022 -62.5120087 -40.4574928 -61.6601334 -39.7562447 -61.2368622 -39.242054 -62.0887375']]\n",
      "['-39.242054 -62.0887375 -39.9433022 -62.5120087 -40.4574928 -61.6601334 -39.7562447 -61.2368622 -39.242054 -62.0887375']\n",
      "['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFLUNCERT_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc']\n",
      "4000\n",
      "[[['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFLUNCERT_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc'], '82', <MULTIPOLYGON (((-62.089 -39.242, -62.512 -39.943, -61.66 -40.457, -61.237 -...>]]\n"
     ]
    }
   ],
   "source": [
    "lon = -62.1123\n",
    "lat = -39.89402\n",
    "point_str = str(lon) +','+ str(lat)\n",
    "\n",
    "page_num = 0\n",
    "page_size = 2000 # CMR page size limit\n",
    "\n",
    "granule_arr = []\n",
    "\n",
    "while True:\n",
    "    page_num += 1\n",
    "     # defining parameters\n",
    "    cmr_param = {\n",
    "        \"collection_concept_id\": concept_id, \n",
    "        \"page_size\": page_size,\n",
    "        \"page_num\": page_num,\n",
    "        \"temporal\": temporal_str,\n",
    "        \"point\":point_str\n",
    "    }\n",
    "\n",
    "    granulesearch = cmrurl + 'granules.json'\n",
    "    response = requests.post(granulesearch, data=cmr_param)\n",
    "    granules = response.json()['feed']['entry']\n",
    "    print(page_num*page_size)  \n",
    "    if granules:\n",
    "        for g in granules:\n",
    "            granule_urls = ''\n",
    "            granule_poly = ''\n",
    "                       \n",
    "            # read cloud cover\n",
    "            cloud_cover = g['cloud_cover']\n",
    "            # reading bounding geometries\n",
    "            if 'polygons' in g:\n",
    "                polygons= g['polygons']\n",
    "                print(polygons)\n",
    "                multipolygons = []\n",
    "                for poly in polygons:\n",
    "                    i=iter(poly[0].split (\" \"))\n",
    "                    ltln = list(map(\" \".join,zip(i,i)))\n",
    "                    multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
    "                granule_poly = MultiPolygon(multipolygons)\n",
    "            # Get https URLs to .nc files and exclude .dmrpp files\n",
    "            granule_urls = [x['href'] for x in g['links'] if 'https' in x['href'] and '.nc' in x['href'] and '.dmrpp' not in x['href']]\n",
    "            granule_arr.append([granule_urls, cloud_cover, granule_poly])\n",
    "                           \n",
    "    else: \n",
    "        break\n",
    "print(granule_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search using a bounding box\n",
    "For this we'll use a bounding box along the coast of Argentina with a bottom left corner of -62.1123 Longitude, -39.89402 Latitude, and a top right corner of -61.70801 Longitude and -39.57769 Latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'feed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m granulesearch \u001b[38;5;241m=\u001b[39m cmrurl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgranules.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(granulesearch, data\u001b[38;5;241m=\u001b[39mcmr_param)\n\u001b[0;32m---> 23\u001b[0m granules \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m granules:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m granules:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feed'"
     ]
    }
   ],
   "source": [
    "# Search Using a Bounding Box\n",
    "bound = (-62.1123, -39.89402, -61.70801, -39.57769) \n",
    "bound_str = ','.join(map(str,bound))\n",
    "\n",
    "page_num = 1\n",
    "page_size = 2000 # CMR page size limit\n",
    "\n",
    "granule_arr = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "     # defining parameters\n",
    "    cmr_param = {\n",
    "        \"collection_concept_id\": concept_id, \n",
    "        \"page_size\": page_size,\n",
    "        \"page_num\": page_num,\n",
    "        \"temporal\": temporal_str,\n",
    "        \"bounding_box[]\":bound_str\n",
    "    }\n",
    "\n",
    "    granulesearch = cmrurl + 'granules.json'\n",
    "    response = requests.post(granulesearch, data=cmr_param)\n",
    "    granules = response.json()['feed']['entry']\n",
    "       \n",
    "    if granules:\n",
    "        for g in granules:\n",
    "            granule_urls = ''\n",
    "            granule_poly = ''\n",
    "                       \n",
    "            # read cloud cover\n",
    "            cloud_cover = g['cloud_cover']\n",
    "    \n",
    "            # reading results bounding geometries\n",
    "            if 'polygons' in g:\n",
    "                polygons= g['polygons']\n",
    "                multipolygons = []\n",
    "                for poly in polygons:\n",
    "                    i=iter(poly[0].split (\" \"))\n",
    "                    ltln = list(map(\" \".join,zip(i,i)))\n",
    "                    multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
    "                granule_poly = MultiPolygon(multipolygons)\n",
    "            \n",
    "            # Get https URLs to .nc files and exclude .dmrpp files\n",
    "            granule_urls = [x['href'] for x in g['links'] if 'https' in x['href'] and '.nc' in x['href'] and '.dmrpp' not in x['href']]\n",
    "            # Add to list\n",
    "            granule_arr.append([granule_urls, cloud_cover, granule_poly])\n",
    "                           \n",
    "    page_num += 1\n",
    "\n",
    "print(granule_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search a Polygon\n",
    "\n",
    "A polygon can also be used to spatially search using the CMR API. A shapefile, geojson, or other format can be opened as a geopandas dataframe, then reformatted to a geojson format to be sent as a parameter in the CMR search. Note that very complex shapefiles must be simplified, there is a 5000 coordinate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "../../data/isla_gaviota.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/ogrext.pyx:136\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../../data/isla_gaviota.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Search using a Polygon\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m polygon \u001b[38;5;241m=\u001b[39m \u001b[43mgeopandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/isla_gaviota.geojson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m geojson \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapefile\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misla_gaviota.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m, polygon\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mto_json(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/geo+json\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m      5\u001b[0m page_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/geopandas/io/file.py:289\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/geopandas/io/file.py:315\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    316\u001b[0m         crs \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fiona/env.py:457\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    454\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fiona/__init__.py:292\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    303\u001b[0m     colxn \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    304\u001b[0m         path,\n\u001b[1;32m    305\u001b[0m         mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fiona/collection.py:243\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:588\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:143\u001b[0m, in \u001b[0;36mfiona.ogrext.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../../data/isla_gaviota.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "# Search using a Polygon\n",
    "polygon = geopandas.read_file('../../data/isla_gaviota.geojson')\n",
    "geojson = {\"shapefile\": (\"isla_gaviota.geojson\", polygon.geometry.to_json(), \"application/geo+json\")}\n",
    "\n",
    "page_num = 1\n",
    "page_size = 2000 # CMR page size limit\n",
    "\n",
    "granule_arr = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "     # defining parameters\n",
    "    cmr_param = {\n",
    "        \"collection_concept_id\": concept_id, \n",
    "        \"page_size\": page_size,\n",
    "        \"page_num\": page_num,\n",
    "        \"temporal\": temporal_str,\n",
    "        \"simplify-shapefile\": 'true' # this is needed to bypass 5000 coordinates limit of CMR\n",
    "    }\n",
    "\n",
    "    granulesearch = cmrurl + 'granules.json'\n",
    "    response = requests.post(granulesearch, data=cmr_param, files=geojson)\n",
    "    granules = response.json()['feed']['entry']\n",
    "       \n",
    "    if granules:\n",
    "        for g in granules:\n",
    "            granule_urls = ''\n",
    "            granule_poly = ''\n",
    "                       \n",
    "            # read granule title and cloud cover\n",
    "            granule_name = g['title']\n",
    "            cloud_cover = g['cloud_cover']\n",
    "    \n",
    "            # reading bounding geometries\n",
    "            if 'polygons' in g:\n",
    "                polygons= g['polygons']\n",
    "                multipolygons = []\n",
    "                for poly in polygons:\n",
    "                    i=iter(poly[0].split (\" \"))\n",
    "                    ltln = list(map(\" \".join,zip(i,i)))\n",
    "                    multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
    "                granule_poly = MultiPolygon(multipolygons)\n",
    "            \n",
    "            # Get https URLs to .nc files and exclude .dmrpp files\n",
    "            granule_urls = [x['href'] for x in g['links'] if 'https' in x['href'] and '.nc' in x['href'] and '.dmrpp' not in x['href']]\n",
    "            # Add to list\n",
    "            granule_arr.append([granule_urls, cloud_cover, granule_poly])\n",
    "                           \n",
    "        page_num += 1\n",
    "    else: \n",
    "        break\n",
    " \n",
    "print(granule_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: At the time this tutorial was made, all 3 searches, point, bounding box, and polygon should result in the same assets being returned.\n",
    "\n",
    "### Creating a Dataframe with the resulting Links\n",
    "\n",
    "A `pandas.dataframe` can be used to store the download URLs and geometries of each file. The EMIT L2A Reflectance and Uncertainty and Mask collection contains 3 assets per granule (reflectance, reflectance uncertainty, and masks). We can see when printing this list, that there are three assets that correspond to a single polygon. For the next step we will place these into a dataframe and 'explode' the dataframe to place each of these in a separate row. If we only want a subset of these assets, we can filter them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_name</th>\n",
       "      <th>asset_url</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>granule_poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc</td>\n",
       "      <td>https://data.lpdaac.earthdatacloud.nasa.gov/lp...</td>\n",
       "      <td>82</td>\n",
       "      <td>MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMIT_L2A_RFLUNCERT_001_20220903T163129_2224611...</td>\n",
       "      <td>https://data.lpdaac.earthdatacloud.nasa.gov/lp...</td>\n",
       "      <td>82</td>\n",
       "      <td>MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc</td>\n",
       "      <td>https://data.lpdaac.earthdatacloud.nasa.gov/lp...</td>\n",
       "      <td>82</td>\n",
       "      <td>MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          asset_name  \\\n",
       "0    EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc   \n",
       "0  EMIT_L2A_RFLUNCERT_001_20220903T163129_2224611...   \n",
       "0   EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc   \n",
       "\n",
       "                                           asset_url cloud_cover  \\\n",
       "0  https://data.lpdaac.earthdatacloud.nasa.gov/lp...          82   \n",
       "0  https://data.lpdaac.earthdatacloud.nasa.gov/lp...          82   \n",
       "0  https://data.lpdaac.earthdatacloud.nasa.gov/lp...          82   \n",
       "\n",
       "                                        granule_poly  \n",
       "0  MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...  \n",
       "0  MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...  \n",
       "0  MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmr_results_df = pd.DataFrame(granule_arr, columns=[\"asset_url\", \"cloud_cover\", \"granule_poly\"])\n",
    "cmr_results_df = cmr_results_df[cmr_results_df['granule_poly'] != '']\n",
    "cmr_results_df = cmr_results_df.explode('asset_url')\n",
    "cmr_results_df.insert(0,'asset_name', cmr_results_df.asset_url.str.split('/',n=-1).str.get(-1))\n",
    "cmr_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_RFLUNCERT_001_20220903T163129_2224611_012.nc', 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20220903T163129_2224611_012/EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc'], '82', <MULTIPOLYGON (((-62.089 -39.242, -62.512 -39.943, -61.66 -40.457, -61.237 -...>]]\n"
     ]
    }
   ],
   "source": [
    "print(granule_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmr_results_df.to_csv(\"ouput.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage we can filter based on the assets that we want or the cloud cover. For this example lets say we are only interested in the Reflectance and the Mask. To filter by asset, we can match strings included in the asset name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_name</th>\n",
       "      <th>asset_url</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>granule_poly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc</td>\n",
       "      <td>https://data.lpdaac.earthdatacloud.nasa.gov/lp...</td>\n",
       "      <td>82</td>\n",
       "      <td>MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc</td>\n",
       "      <td>https://data.lpdaac.earthdatacloud.nasa.gov/lp...</td>\n",
       "      <td>82</td>\n",
       "      <td>MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         asset_name  \\\n",
       "0   EMIT_L2A_RFL_001_20220903T163129_2224611_012.nc   \n",
       "0  EMIT_L2A_MASK_001_20220903T163129_2224611_012.nc   \n",
       "\n",
       "                                           asset_url cloud_cover  \\\n",
       "0  https://data.lpdaac.earthdatacloud.nasa.gov/lp...          82   \n",
       "0  https://data.lpdaac.earthdatacloud.nasa.gov/lp...          82   \n",
       "\n",
       "                                        granule_poly  \n",
       "0  MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...  \n",
       "0  MULTIPOLYGON (((-62.0887375 -39.242054, -62.51...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmr_results_df = cmr_results_df[cmr_results_df.asset_name.str.contains('_RFL_') | cmr_results_df.asset_name.str.contains('MASK')]\n",
    "cmr_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering down to the assets you want, you can output a text file with the asset urls or save the entire dataframe, then use a utility such as wget or the DAAC Data Download Tool to download the files. To download you will need to set up NASA Earthdata Login authentication using  a .netrc file. \n",
    "\n",
    "Save the asset urls to a textfile in the `/data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 0-1: truncated \\UXXXXXXXX escape (3420063674.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    cmr_results_df.to_csv('\\Users\\arpitasen\\Desktop\\Sanghavi\\Python', columns = ['asset_url'], index=False, header = False)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 0-1: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Save text file of asset urls\n",
    "cmr_results_dfs = cmr_results_df[:-1].drop_duplicates(subset=['asset_url']) # Remove any duplicates\n",
    "cmr_results_df.to_csv('../../data/emit_asset_urls.txt', columns = ['asset_url'], index=False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Files using the list of URLS/Text File\n",
    "\n",
    "To download the files using Python, you can run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/emit_asset_urls.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Open Text file\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43murl_list_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      8\u001b[0m         file_list \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[1;32m      9\u001b[0m         file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/emit_asset_urls.txt'"
     ]
    }
   ],
   "source": [
    "# Define input filepath\n",
    "url_list_filepath = \"../../data/emit_asset_urls.txt\"\n",
    "# Define output directory\n",
    "output_directory = \"../../data/\"\n",
    "\n",
    "# Open Text file\n",
    "with open(url_list_filepath, \"r\") as file:\n",
    "        file_list = file.read().splitlines()\n",
    "        file.close()\n",
    "\n",
    "# EDL Authentication/Create .netrc if necessary\n",
    "earthaccess.login(persist=True)\n",
    "# Get requests https Session using Earthdata Login Info\n",
    "fs = earthaccess.get_requests_https_session()\n",
    "# Retrieve granule asset ID from URL (to maintain existing naming convention)\n",
    "for url in file_list:\n",
    "    granule_asset_id = url.split(\"/\")[-1]\n",
    "    # Define Local Filepath\n",
    "    fp = f\"{output_directory}{granule_asset_id}\"\n",
    "    # Download the Granule Asset if it doesn't exist\n",
    "    print(f\"Downloading {granule_asset_id}...\")\n",
    "    if not os.path.isfile(fp):\n",
    "        with fs.get(url, stream=True) as src:\n",
    "            with open(fp, \"wb\") as dst:\n",
    "                for chunk in src.iter_content(chunk_size=64 * 1024 * 1024):\n",
    "                    dst.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download using wget, use the following in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -P ../../data/ -i ../../data/emit_asset_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact Info:  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "Date last modified: 03-22-2024  \n",
    "\n",
    "¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "3292b2aceff7d39327a7519422d4180a7c9b133202090f26e797e3dd8f2c7877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
